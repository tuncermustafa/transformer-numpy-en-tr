{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNjc1XH7qRdDui2yVeW4OXt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h7oxPyuXL-we","executionInfo":{"status":"ok","timestamp":1749149197344,"user_tz":-180,"elapsed":17571866,"user":{"displayName":"Mustafa TunÃ§er","userId":"09475384975611862473"}},"outputId":"b81b23ba-a710-4147-8c3f-afa804738e00"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","KullanÄ±lacak cihaz: cuda\n","'/content/drive/My Drive/transformer/en2tr_train.txt' dosyasÄ±ndan 425731 cÃ¼mle Ã§ifti yÃ¼klendi.\n","'/content/drive/My Drive/transformer/en2tr_valid.txt' dosyasÄ±ndan 47304 cÃ¼mle Ã§ifti yÃ¼klendi.\n","Kelime daÄŸarcÄ±ÄŸÄ± oluÅŸturuldu. Eklenen yeni kelime: 14587. Toplam boyut: 14591\n","Kelime daÄŸarcÄ±ÄŸÄ± oluÅŸturuldu. Eklenen yeni kelime: 47647. Toplam boyut: 47651\n","EÄŸitime 1. epoch'tan baÅŸlanÄ±yor...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n","  output = torch._nested_tensor_from_mask(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/100 | SÃ¼re: 529.46s | EÄŸitim K: 5.6817 | Val K: 4.2346 | LR: 1.00e-04\n","  âœ¨ Yeni en iyi model kaydedildi: best_model_512.pth (Val K: 4.2346)\n","  Checkpoint kaydedildi: /content/drive/My Drive/transformer/model_checkpoints/last_checkpoint_512.pth\n","Epoch 2/100 | SÃ¼re: 527.88s | EÄŸitim K: 3.8003 | Val K: 3.0192 | LR: 1.00e-04\n","  âœ¨ Yeni en iyi model kaydedildi: best_model_512.pth (Val K: 3.0192)\n","Epoch 3/100 | SÃ¼re: 527.95s | EÄŸitim K: 2.8673 | Val K: 2.3483 | LR: 1.00e-04\n","  âœ¨ Yeni en iyi model kaydedildi: best_model_512.pth (Val K: 2.3483)\n","Epoch 4/100 | SÃ¼re: 527.94s | EÄŸitim K: 2.3113 | Val K: 1.9492 | LR: 1.00e-04\n","  âœ¨ Yeni en iyi model kaydedildi: best_model_512.pth (Val K: 1.9492)\n","Epoch 5/100 | SÃ¼re: 527.84s | EÄŸitim K: 1.9583 | Val K: 1.7107 | LR: 1.00e-04\n","  âœ¨ Yeni en iyi model kaydedildi: best_model_512.pth (Val K: 1.7107)\n","Epoch 6/100 | SÃ¼re: 528.00s | EÄŸitim K: 1.7162 | Val K: 1.5438 | LR: 1.00e-04\n","  âœ¨ Yeni en iyi model kaydedildi: best_model_512.pth (Val K: 1.5438)\n","Epoch 7/100 | SÃ¼re: 527.93s | EÄŸitim K: 1.5383 | Val K: 1.4265 | LR: 1.00e-04\n","  âœ¨ Yeni en iyi model kaydedildi: best_model_512.pth (Val K: 1.4265)\n","Epoch 8/100 | SÃ¼re: 528.30s | EÄŸitim K: 1.4047 | Val K: 1.3415 | LR: 1.00e-04\n","  âœ¨ Yeni en iyi model kaydedildi: best_model_512.pth (Val K: 1.3415)\n","Epoch 9/100 | SÃ¼re: 528.52s | EÄŸitim K: 1.2967 | Val K: 1.2798 | LR: 1.00e-04\n","  âœ¨ Yeni en iyi model kaydedildi: best_model_512.pth (Val K: 1.2798)\n","Epoch 10/100 | SÃ¼re: 528.46s | EÄŸitim K: 1.2096 | Val K: 1.2444 | LR: 1.00e-04\n","  âœ¨ Yeni en iyi model kaydedildi: best_model_512.pth (Val K: 1.2444)\n","  Checkpoint kaydedildi: /content/drive/My Drive/transformer/model_checkpoints/last_checkpoint_512.pth\n","Epoch 11/100 | SÃ¼re: 528.26s | EÄŸitim K: 1.1362 | Val K: 1.1929 | LR: 1.00e-04\n","  âœ¨ Yeni en iyi model kaydedildi: best_model_512.pth (Val K: 1.1929)\n","Epoch 12/100 | SÃ¼re: 528.61s | EÄŸitim K: 1.0727 | Val K: 1.1616 | LR: 1.00e-04\n","  âœ¨ Yeni en iyi model kaydedildi: best_model_512.pth (Val K: 1.1616)\n","Epoch 13/100 | SÃ¼re: 528.15s | EÄŸitim K: 1.0182 | Val K: 1.1492 | LR: 1.00e-04\n","  âœ¨ Yeni en iyi model kaydedildi: best_model_512.pth (Val K: 1.1492)\n","Epoch 14/100 | SÃ¼re: 528.41s | EÄŸitim K: 0.9688 | Val K: 1.1212 | LR: 1.00e-04\n","  âœ¨ Yeni en iyi model kaydedildi: best_model_512.pth (Val K: 1.1212)\n","Epoch 15/100 | SÃ¼re: 528.48s | EÄŸitim K: 0.9245 | Val K: 1.1061 | LR: 1.00e-04\n","  âœ¨ Yeni en iyi model kaydedildi: best_model_512.pth (Val K: 1.1061)\n","Epoch 16/100 | SÃ¼re: 528.40s | EÄŸitim K: 0.8839 | Val K: 1.1109 | LR: 1.00e-04\n","Epoch 17/100 | SÃ¼re: 528.62s | EÄŸitim K: 0.8470 | Val K: 1.0978 | LR: 1.00e-04\n","  âœ¨ Yeni en iyi model kaydedildi: best_model_512.pth (Val K: 1.0978)\n","Epoch 18/100 | SÃ¼re: 528.44s | EÄŸitim K: 0.8139 | Val K: 1.0886 | LR: 1.00e-04\n","  âœ¨ Yeni en iyi model kaydedildi: best_model_512.pth (Val K: 1.0886)\n","Epoch 19/100 | SÃ¼re: 528.67s | EÄŸitim K: 0.7834 | Val K: 1.0877 | LR: 1.00e-04\n","Epoch 20/100 | SÃ¼re: 528.56s | EÄŸitim K: 0.7543 | Val K: 1.1092 | LR: 1.00e-04\n","  Checkpoint kaydedildi: /content/drive/My Drive/transformer/model_checkpoints/last_checkpoint_512.pth\n","Epoch 21/100 | SÃ¼re: 528.68s | EÄŸitim K: 0.7282 | Val K: 1.1081 | LR: 1.00e-04\n","Epoch 22/100 | SÃ¼re: 528.68s | EÄŸitim K: 0.7040 | Val K: 1.0884 | LR: 1.00e-04\n","Epoch 23/100 | SÃ¼re: 528.61s | EÄŸitim K: 0.6804 | Val K: 1.1246 | LR: 1.00e-04\n","Epoch 24/100 | SÃ¼re: 528.65s | EÄŸitim K: 0.6593 | Val K: 1.1139 | LR: 1.00e-04\n","Epoch 25/100 | SÃ¼re: 528.51s | EÄŸitim K: 0.6392 | Val K: 1.1154 | LR: 1.00e-04\n","  Epoch 25: Ã–ÄŸrenme oranÄ± 1.00e-04 -> 1.00e-05\n","Epoch 26/100 | SÃ¼re: 527.96s | EÄŸitim K: 0.5449 | Val K: 1.1089 | LR: 1.00e-05\n","Epoch 27/100 | SÃ¼re: 528.14s | EÄŸitim K: 0.5281 | Val K: 1.1123 | LR: 1.00e-05\n","Epoch 28/100 | SÃ¼re: 528.03s | EÄŸitim K: 0.5199 | Val K: 1.1097 | LR: 1.00e-05\n","Epoch 29/100 | SÃ¼re: 527.96s | EÄŸitim K: 0.5149 | Val K: 1.1219 | LR: 1.00e-05\n","Epoch 30/100 | SÃ¼re: 527.96s | EÄŸitim K: 0.5100 | Val K: 1.1213 | LR: 1.00e-05\n","  Checkpoint kaydedildi: /content/drive/My Drive/transformer/model_checkpoints/last_checkpoint_512.pth\n","Epoch 31/100 | SÃ¼re: 528.02s | EÄŸitim K: 0.5054 | Val K: 1.1258 | LR: 1.00e-05\n","  Epoch 31: Ã–ÄŸrenme oranÄ± 1.00e-05 -> 1.00e-06\n","Epoch 32/100 | SÃ¼re: 528.20s | EÄŸitim K: 0.4942 | Val K: 1.1229 | LR: 1.00e-06\n","Epoch 33/100 | SÃ¼re: 528.15s | EÄŸitim K: 0.4938 | Val K: 1.1213 | LR: 1.00e-06\n","ðŸ”´ Erken Durdurma: Val kaybÄ± 15 epoch boyunca iyileÅŸmedi. En iyi Val K: 1.0886\n","\n","âœ… EÄŸitim TamamlandÄ±.\n","Son checkpoint: /content/drive/My Drive/transformer/model_checkpoints/last_checkpoint_512.pth\n","En iyi model 'best_model_512.pth': /content/drive/My Drive/transformer/model_checkpoints/best_model_512.pth (Val K: 1.0886)\n","\n","DoÄŸrulama Veri Setinden Rastgele Ã–rneklerle Ã‡Ä±karÄ±m (En Ä°yi Model KullanÄ±larak)\n","En iyi model '/content/drive/My Drive/transformer/model_checkpoints/best_model_512.pth' yÃ¼kleniyor...\n","\n","Ã–rnek 1 (Validasyon setinden 41905. indeks):\n","  Kaynak (Ä°ngilizce): why should it be different\n","  Referans (TÃ¼rkÃ§e) : neden farklÄ± olmalÄ±\n","  Model Ã‡evirisi    : bu neden farklÄ± olmalÄ±\n","\n","Ã–rnek 2 (Validasyon setinden 7296. indeks):\n","  Kaynak (Ä°ngilizce): i'm going to speak to tom\n","  Referans (TÃ¼rkÃ§e) : tom'la konuÅŸacaÄŸÄ±m\n","  Model Ã‡evirisi    : tom'la konuÅŸacaÄŸÄ±m\n","\n","Ã–rnek 3 (Validasyon setinden 1639. indeks):\n","  Kaynak (Ä°ngilizce): i should've declined\n","  Referans (TÃ¼rkÃ§e) : reddetmeliydim\n","  Model Ã‡evirisi    : <unk>\n","\n","Ã–rnek 4 (Validasyon setinden 18024. indeks):\n","  Kaynak (Ä°ngilizce): you're canadian aren't you\n","  Referans (TÃ¼rkÃ§e) : kanadalÄ±sÄ±n deÄŸil mi\n","  Model Ã‡evirisi    : sen kanadalÄ±sÄ±n deÄŸil mi\n","\n","Ã–rnek 5 (Validasyon setinden 16049. indeks):\n","  Kaynak (Ä°ngilizce): being a single parent isn't easy\n","  Referans (TÃ¼rkÃ§e) : tek bir ebeveyn olmak kolay deÄŸildir\n","  Model Ã‡evirisi    : bekar olmak kolay deÄŸildir\n"]}],"source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import os\n","import random\n","import time\n","from typing import List, Tuple\n","\n","# Google Drive'a baÄŸlanma\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Sabitler ve Hiperparametreler\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"KullanÄ±lacak cihaz: {DEVICE}\")\n","\n","# Model Mimarisi Parametreleri\n","D_MODEL = 512\n","N_HEAD = 8\n","NUM_ENCODER_LAYERS = 6\n","NUM_DECODER_LAYERS = 6\n","DIM_FEEDFORWARD = 1024\n","DROPOUT = 0.1\n","ACTIVATION_FN_STR = \"relu\" # nn.TransformerEncoderLayer/DecoderLayer bunu doÄŸrudan kabul eder\n","MAX_SEQ_LEN = 50\n","\n","# EÄŸitim Hiperparametreleri\n","BATCH_SIZE = 512\n","LEARNING_RATE = 0.0001\n","NUM_EPOCHS = 100\n","CLIP_GRAD = 1.0\n","MIN_VOCAB_FREQ = 2\n","SAVE_LOG_INTERVAL = 1\n","\n","# Erken Durdurma Parametreleri\n","EARLY_STOPPING_PATIENCE = 10\n","EARLY_STOPPING_MIN_DELTA = 0.001\n","\n","# LR Scheduler Parametreleri\n","SCHEDULER_PATIENCE = 5\n","SCHEDULER_FACTOR = 0.1\n","\n","# Dosya YollarÄ±\n","DRIVE_BASE_PATH = '/content/drive/My Drive/transformer/'\n","TRAIN_DATA_FILE_NAME = 'en2tr_train.txt'\n","VALID_DATA_FILE_NAME = 'en2tr_valid.txt'\n","MODEL_CHECKPOINT_DIR_NAME = 'model_checkpoints'\n","LAST_CHECKPOINT_FILE_NAME = 'last_checkpoint_512.pth'\n","BEST_MODEL_FILE_NAME = 'best_model_512.pth'\n","\n","TRAIN_FILE_PATH = os.path.join(DRIVE_BASE_PATH, TRAIN_DATA_FILE_NAME)\n","VALID_FILE_PATH = os.path.join(DRIVE_BASE_PATH, VALID_DATA_FILE_NAME)\n","CHECKPOINT_DIR = os.path.join(DRIVE_BASE_PATH, MODEL_CHECKPOINT_DIR_NAME)\n","os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n","\n","BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, BEST_MODEL_FILE_NAME)\n","LAST_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, LAST_CHECKPOINT_FILE_NAME)\n","\n","RANDOM_SEED = 42\n","random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","if DEVICE.type == 'cuda':\n","    torch.cuda.manual_seed_all(RANDOM_SEED)\n","\n","# SINIFLAR\n","class WordTokenizer:\n","    def __init__(self):\n","        self.word2idx = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n","        self.idx2word = {v: k for k, v in self.word2idx.items()}\n","        self.vocab_size = len(self.word2idx)\n","        self.min_freq = 1\n","\n","    def build_vocab(self, sentences: List[str], min_freq: int = 1):\n","        self.min_freq = min_freq\n","        word_counts = {}\n","        for sentence in sentences:\n","            for word in sentence.split():\n","                word_counts[word] = word_counts.get(word, 0) + 1\n","\n","        newly_added_count = 0\n","        for word, count in word_counts.items():\n","            if count >= self.min_freq and word not in self.word2idx:\n","                idx = len(self.word2idx)\n","                self.word2idx[word] = idx\n","                self.idx2word[idx] = word\n","                newly_added_count +=1\n","        self.vocab_size = len(self.word2idx)\n","        print(f\"Kelime daÄŸarcÄ±ÄŸÄ± oluÅŸturuldu. Eklenen yeni kelime: {newly_added_count}. Toplam boyut: {self.vocab_size}\")\n","\n","    def encode(self, sentence: str, add_sos_eos: bool = False) -> List[int]:\n","        tokens = [self.word2idx.get(word, self.word2idx['<unk>']) for word in sentence.split()]\n","        if add_sos_eos:\n","            tokens = [self.word2idx['<sos>']] + tokens + [self.word2idx['<eos>']]\n","        return tokens\n","\n","    def decode(self, indices: List[int]) -> str:\n","        return ' '.join([self.idx2word.get(idx, '<unk>') for idx in indices if idx not in [self.word2idx['<pad>'], self.word2idx['<sos>'], self.word2idx['<eos>']]])\n","\n","class Embedding(nn.Module):\n","    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int = None):\n","        super().__init__()\n","        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n","        self.embedding_dim = embedding_dim\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return self.embedding(x)\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=100):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        if d_model % 2 != 0:\n","            pe[:, 1::2] = torch.cos(position * div_term)[:,:d_model//2]\n","        else:\n","            pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1)]\n","        return self.dropout(x)\n","\n","class Transformer(nn.Module):\n","    def __init__(self, src_vocab_size: int, trg_vocab_size: int,\n","                 src_pad_idx: int, trg_pad_idx: int,\n","                 d_model: int, nhead: int,\n","                 num_encoder_layers: int, num_decoder_layers: int,\n","                 dim_feedforward: int, dropout: float,\n","                 activation_fn_str: str, max_seq_len: int):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.src_pad_idx = src_pad_idx\n","        self.trg_pad_idx = trg_pad_idx\n","\n","        self.src_embedding = Embedding(src_vocab_size, d_model, padding_idx=src_pad_idx)\n","        self.trg_embedding = Embedding(trg_vocab_size, d_model, padding_idx=trg_pad_idx)\n","        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=max_seq_len)\n","\n","        self.transformer = nn.Module()\n","\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=d_model,\n","            nhead=nhead,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout,\n","            activation=activation_fn_str, # \"relu\" veya \"gelu\"\n","            batch_first=True,\n","            norm_first=False # Orijinal kod post-norm idi\n","        )\n","        encoder_norm = nn.LayerNorm(d_model) # Encoder Ã§Ä±kÄ±ÅŸÄ±na uygulanacak norm\n","        self.transformer.encoder = nn.TransformerEncoder(\n","            encoder_layer,\n","            num_layers=num_encoder_layers,\n","            norm=encoder_norm\n","        )\n","\n","        decoder_layer = nn.TransformerDecoderLayer(\n","            d_model=d_model,\n","            nhead=nhead,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout,\n","            activation=activation_fn_str, # \"relu\" veya \"gelu\"\n","            batch_first=True,\n","            norm_first=False\n","        )\n","        decoder_norm = nn.LayerNorm(d_model) # Decoder Ã§Ä±kÄ±ÅŸÄ±na uygulanacak norm\n","        self.transformer.decoder = nn.TransformerDecoder(\n","            decoder_layer,\n","            num_layers=num_decoder_layers,\n","            norm=decoder_norm\n","        )\n","\n","        self.fc_out = nn.Linear(d_model, trg_vocab_size)\n","        self._initialize_weights()\n","\n","    def _initialize_weights(self):\n","        for name, p in self.named_parameters():\n","            if p.dim() > 1:\n","                nn.init.xavier_uniform_(p)\n","        if self.src_embedding.embedding.padding_idx is not None:\n","            with torch.no_grad():\n","                self.src_embedding.embedding.weight[self.src_embedding.embedding.padding_idx].fill_(0)\n","        if self.trg_embedding.embedding.padding_idx is not None:\n","            with torch.no_grad():\n","                self.trg_embedding.embedding.weight[self.trg_embedding.embedding.padding_idx].fill_(0)\n","\n","    def generate_square_subsequent_mask(self, sz: int, device: torch.device) -> torch.Tensor:\n","        return torch.triu(torch.full((sz, sz), float('-inf'), device=device, dtype=torch.float32), diagonal=1)\n","\n","    def create_padding_mask(self, seq: torch.Tensor, pad_idx: int) -> torch.Tensor:\n","        return (seq == pad_idx) # (batch_size, seq_len), True olan yerler pad\n","\n","    def forward(self, src: torch.Tensor, trg_input: torch.Tensor) -> torch.Tensor:\n","        src_emb_unencoded = self.src_embedding(src) * math.sqrt(self.d_model)\n","        trg_emb_unencoded = self.trg_embedding(trg_input) * math.sqrt(self.d_model)\n","\n","        src_emb = self.pos_encoder(src_emb_unencoded)\n","        trg_emb = self.pos_encoder(trg_emb_unencoded)\n","\n","        # Boolean padding maskeleri\n","        src_key_padding_mask_bool = self.create_padding_mask(src, self.src_pad_idx)\n","        trg_key_padding_mask_bool = self.create_padding_mask(trg_input, self.trg_pad_idx)\n","\n","        # Float nedensel maske (decoder self-attention iÃ§in)\n","        tgt_attn_mask_float = self.generate_square_subsequent_mask(trg_input.size(1), device=trg_input.device)\n","\n","        # Boolean padding maskelerini float'a Ã§evir (0.0 unmasked, -inf masked)\n","        # dtype ve device'Ä± dikkat edilecek tensÃ¶rlerden al\n","        src_key_padding_mask_float = torch.zeros_like(src_key_padding_mask_bool, dtype=src_emb.dtype, device=src_emb.device)\n","        src_key_padding_mask_float.masked_fill_(src_key_padding_mask_bool, float('-inf'))\n","\n","        trg_key_padding_mask_float = torch.zeros_like(trg_key_padding_mask_bool, dtype=trg_emb.dtype, device=trg_emb.device)\n","        trg_key_padding_mask_float.masked_fill_(trg_key_padding_mask_bool, float('-inf'))\n","\n","        memory = self.transformer.encoder(\n","            src=src_emb,\n","            mask=None,\n","            src_key_padding_mask=src_key_padding_mask_float # Float maske\n","        )\n","\n","        output = self.transformer.decoder(\n","            tgt=trg_emb,\n","            memory=memory,\n","            tgt_mask=tgt_attn_mask_float, # Float nedensel maske\n","            memory_mask=None,\n","            tgt_key_padding_mask=trg_key_padding_mask_float, # Float padding maskesi\n","            memory_key_padding_mask=src_key_padding_mask_float # Float padding maskesi (kaynaktan)\n","        )\n","\n","        return self.fc_out(output)\n","\n","# Veri HazÄ±rlÄ±ÄŸÄ±\n","def load_cleaned_sentence_pairs(filepath: str) -> List[Tuple[str, str]]:\n","    sentence_pairs = []\n","    try:\n","        with open(filepath, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                parts = line.strip().split('\\t')\n","                if len(parts) == 2:\n","                    src_sentence, trg_sentence = parts[0].strip(), parts[1].strip()\n","                    if src_sentence and trg_sentence:\n","                        sentence_pairs.append((src_sentence, trg_sentence))\n","    except FileNotFoundError:\n","        print(f\"HATA: Veri dosyasÄ± '{filepath}' bulunamadÄ±.\")\n","        return []\n","    print(f\"'{filepath}' dosyasÄ±ndan {len(sentence_pairs)} cÃ¼mle Ã§ifti yÃ¼klendi.\")\n","    return sentence_pairs\n","\n","class TranslationDataset(Dataset):\n","    def __init__(self, sentence_pairs: List[Tuple[str, str]], src_tokenizer: WordTokenizer, trg_tokenizer: WordTokenizer):\n","        self.sentence_pairs = sentence_pairs\n","        self.src_tokenizer = src_tokenizer\n","        self.trg_tokenizer = trg_tokenizer\n","\n","    def __len__(self): return len(self.sentence_pairs)\n","\n","    def __getitem__(self, idx) -> Tuple[List[int], List[int]]:\n","        src_raw, trg_raw = self.sentence_pairs[idx]\n","        src_tokens = self.src_tokenizer.encode(src_raw, add_sos_eos=True)\n","        trg_tokens = self.trg_tokenizer.encode(trg_raw, add_sos_eos=True)\n","        return src_tokens, trg_tokens\n","\n","def collate_fn(batch: List[Tuple[List[int], List[int]]], pad_idx: int, max_len: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","    src_batch_tokens, trg_batch_tokens = [], []\n","    for src_item_tokens, trg_item_tokens in batch:\n","        src_processed = src_item_tokens[:max_len]\n","        trg_processed = trg_item_tokens[:max_len]\n","        src_padded = src_processed + [pad_idx] * (max_len - len(src_processed))\n","        trg_padded = trg_processed + [pad_idx] * (max_len - len(trg_processed))\n","        src_batch_tokens.append(src_padded)\n","        trg_batch_tokens.append(trg_padded)\n","\n","    src_tensor = torch.LongTensor(src_batch_tokens)\n","    trg_tensor = torch.LongTensor(trg_batch_tokens)\n","    trg_input = trg_tensor[:, :-1]\n","    trg_output = trg_tensor[:, 1:]\n","    return src_tensor, trg_input, trg_output\n","\n","SRC_TOKENIZER = WordTokenizer()\n","TRG_TOKENIZER = WordTokenizer()\n","PAD_IDX = SRC_TOKENIZER.word2idx['<pad>'] # Sabit: 0\n","\n","current_d_model, current_n_head, current_num_encoder_layers, current_num_decoder_layers, \\\n","current_dim_feedforward, current_dropout, current_activation_fn_str, current_max_seq_len = \\\n","D_MODEL, N_HEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, \\\n","DIM_FEEDFORWARD, DROPOUT, ACTIVATION_FN_STR, MAX_SEQ_LEN\n","\n","# Veri YÃ¼kleme\n","train_pairs = load_cleaned_sentence_pairs(TRAIN_FILE_PATH)\n","val_pairs = load_cleaned_sentence_pairs(VALID_FILE_PATH)\n","\n","if not train_pairs:\n","    print(f\"EÄŸitim verisi ({TRAIN_FILE_PATH}) yÃ¼klenemedi, program durduruluyor.\")\n","    exit()\n","\n","# Tokenizer'larÄ± eÄŸitim verisinden oluÅŸtur\n","src_train_sentences = [pair[0] for pair in train_pairs]\n","trg_train_sentences = [pair[1] for pair in train_pairs]\n","SRC_TOKENIZER.build_vocab(src_train_sentences, min_freq=MIN_VOCAB_FREQ)\n","TRG_TOKENIZER.build_vocab(trg_train_sentences, min_freq=MIN_VOCAB_FREQ)\n","\n","model = Transformer(SRC_TOKENIZER.vocab_size, TRG_TOKENIZER.vocab_size,\n","                    PAD_IDX, PAD_IDX,\n","                    current_d_model, current_n_head,\n","                    current_num_encoder_layers, current_num_decoder_layers,\n","                    current_dim_feedforward, current_dropout,\n","                    current_activation_fn_str, current_max_seq_len).to(DEVICE)\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=SCHEDULER_FACTOR, patience=SCHEDULER_PATIENCE)\n","\n","# Veri YÃ¼kleyiciler\n","train_dataset = TranslationDataset(train_pairs, SRC_TOKENIZER, TRG_TOKENIZER)\n","val_dataset = TranslationDataset(val_pairs, SRC_TOKENIZER, TRG_TOKENIZER) if val_pairs else None\n","\n","collate_with_params = lambda batch: collate_fn(batch, PAD_IDX, current_max_seq_len)\n","num_avail_workers = os.cpu_count() if hasattr(os, 'cpu_count') else 0\n","num_workers_to_use = num_avail_workers // 2 if num_avail_workers > 1 else 0\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n","                              collate_fn=collate_with_params, num_workers=num_workers_to_use,\n","                              pin_memory=DEVICE.type == 'cuda', persistent_workers=num_workers_to_use > 0)\n","val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n","                            collate_fn=collate_with_params, num_workers=num_workers_to_use,\n","                            pin_memory=DEVICE.type == 'cuda', persistent_workers=num_workers_to_use > 0) if val_dataset else None\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","\n","# EÄŸitim DÃ¶ngÃ¼sÃ¼ FonksiyonlarÄ±\n","def train_epoch(model_train, dataloader_train, optimizer_train, criterion_train, clip_grad_train):\n","    model_train.train()\n","    total_loss = 0\n","    num_batches = len(dataloader_train)\n","    if num_batches == 0: return 0.0\n","\n","    for src_batch, trg_input_batch, trg_output_batch in dataloader_train:\n","        src_batch, trg_input_batch, trg_output_batch = \\\n","            src_batch.to(DEVICE, non_blocking=True), \\\n","            trg_input_batch.to(DEVICE, non_blocking=True), \\\n","            trg_output_batch.to(DEVICE, non_blocking=True)\n","\n","        optimizer_train.zero_grad(set_to_none=True)\n","        output_logits = model_train(src_batch, trg_input_batch)\n","        loss = criterion_train(output_logits.reshape(-1, output_logits.shape[-1]), trg_output_batch.reshape(-1))\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model_train.parameters(), clip_grad_train)\n","        optimizer_train.step()\n","        total_loss += loss.item()\n","    return total_loss / num_batches\n","\n","def evaluate(model_eval, dataloader_eval, criterion_eval):\n","    model_eval.eval()\n","    total_loss = 0\n","    num_batches = len(dataloader_eval)\n","    if num_batches == 0: return float('inf')\n","\n","    with torch.no_grad():\n","        for src_batch, trg_input_batch, trg_output_batch in dataloader_eval:\n","            src_batch, trg_input_batch, trg_output_batch = \\\n","                src_batch.to(DEVICE, non_blocking=True), \\\n","                trg_input_batch.to(DEVICE, non_blocking=True), \\\n","                trg_output_batch.to(DEVICE, non_blocking=True)\n","            output_logits = model_eval(src_batch, trg_input_batch)\n","            loss = criterion_eval(output_logits.reshape(-1, output_logits.shape[-1]), trg_output_batch.reshape(-1))\n","            total_loss += loss.item()\n","    return total_loss / num_batches\n","\n","# Ã‡Ä±karÄ±m FonksiyonlarÄ±\n","def translate_single_sentence_for_validation(\n","    inference_model: Transformer,\n","    sentence_str: str,\n","    src_tokenizer_inf: WordTokenizer,\n","    trg_tokenizer_inf: WordTokenizer,\n","    device_inf: torch.device,\n","    max_len_inf: int):\n","\n","    inference_model.eval()\n","    with torch.no_grad():\n","        cleaned_sentence = sentence_str.strip().lower()\n","        src_tokens = src_tokenizer_inf.encode(cleaned_sentence, add_sos_eos=True)\n","        if len(src_tokens) > max_len_inf:\n","            src_tokens = src_tokens[:max_len_inf-1] + [src_tokenizer_inf.word2idx['<eos>']]\n","        src_tensor = torch.LongTensor(src_tokens).unsqueeze(0).to(device_inf)\n","\n","        src_emb_unencoded = inference_model.src_embedding(src_tensor) * math.sqrt(inference_model.d_model)\n","        src_pos_emb = inference_model.pos_encoder(src_emb_unencoded) # Positional encoding sonrasÄ±\n","\n","        src_key_padding_mask_bool = inference_model.create_padding_mask(src_tensor, inference_model.src_pad_idx)\n","        # Float padding maskesi oluÅŸtur\n","        src_key_padding_mask_float = torch.zeros_like(src_key_padding_mask_bool, dtype=src_pos_emb.dtype, device=src_pos_emb.device)\n","        src_key_padding_mask_float.masked_fill_(src_key_padding_mask_bool, float('-inf'))\n","\n","        memory = inference_model.transformer.encoder(\n","            src=src_pos_emb,\n","            mask=None, # Encoder iÃ§in genelde src_mask None olur\n","            src_key_padding_mask=src_key_padding_mask_float # Float maske\n","        )\n","\n","        trg_indices = [trg_tokenizer_inf.word2idx['<sos>']]\n","        for _ in range(max_len_inf):\n","            trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(device_inf)\n","\n","            trg_emb_unencoded = inference_model.trg_embedding(trg_tensor) * math.sqrt(inference_model.d_model)\n","            trg_pos_emb = inference_model.pos_encoder(trg_emb_unencoded) # Positional encoding sonrasÄ±\n","\n","            tgt_causal_mask_float = inference_model.generate_square_subsequent_mask(trg_tensor.size(1), device_inf) # Bu zaten float\n","\n","            trg_key_padding_mask_bool = inference_model.create_padding_mask(trg_tensor, inference_model.trg_pad_idx)\n","            # Float padding maskesi oluÅŸtur\n","            trg_key_padding_mask_float = torch.zeros_like(trg_key_padding_mask_bool, dtype=trg_pos_emb.dtype, device=trg_pos_emb.device)\n","            trg_key_padding_mask_float.masked_fill_(trg_key_padding_mask_bool, float('-inf'))\n","\n","            output_decoder = inference_model.transformer.decoder(\n","                tgt=trg_pos_emb,\n","                memory=memory,\n","                tgt_mask=tgt_causal_mask_float, # Float nedensel maske\n","                memory_mask=None, # Cross-attention iÃ§in memory Ã¼zerinde ek maske (opsiyonel)\n","                tgt_key_padding_mask=trg_key_padding_mask_float, # Float padding maskesi\n","                memory_key_padding_mask=src_key_padding_mask_float # Float padding maskesi (kaynaktan)\n","            )\n","            pred_token_logits = inference_model.fc_out(output_decoder[:, -1])\n","            pred_token = pred_token_logits.argmax(1).item()\n","            trg_indices.append(pred_token)\n","            if pred_token == trg_tokenizer_inf.word2idx['<eos>']: break\n","        return trg_tokenizer_inf.decode(trg_indices)\n","\n","def run_inference_on_validation_samples(\n","    num_samples_to_infer: int, validation_pairs_inf: List[Tuple[str,str]], best_model_weights_path: str,\n","    src_tokenizer_global: WordTokenizer, trg_tokenizer_global: WordTokenizer, pad_idx_global: int,\n","    d_model_global: int, n_head_global: int, num_encoder_layers_global: int, num_decoder_layers_global: int,\n","    dim_feedforward_global: int, dropout_global: float, activation_fn_str_global: str, max_seq_len_global: int,\n","    device_global: torch.device):\n","\n","    print(\"\\nDoÄŸrulama Veri Setinden Rastgele Ã–rneklerle Ã‡Ä±karÄ±m (En Ä°yi Model KullanÄ±larak)\")\n","    if not os.path.exists(best_model_weights_path):\n","        print(f\"En iyi model dosyasÄ± bulunamadÄ±: {best_model_weights_path}\\nÃ‡Ä±karÄ±m yapÄ±lamÄ±yor.\")\n","        return\n","    if not validation_pairs_inf:\n","        print(\"DoÄŸrulama veri seti boÅŸ. Ã‡Ä±karÄ±m yapÄ±lamÄ±yor.\")\n","        return\n","\n","    print(f\"En iyi model '{best_model_weights_path}' yÃ¼kleniyor...\")\n","    inference_model = Transformer(\n","        src_tokenizer_global.vocab_size, trg_tokenizer_global.vocab_size,\n","        pad_idx_global, pad_idx_global,\n","        d_model_global, n_head_global,\n","        num_encoder_layers_global, num_decoder_layers_global,\n","        dim_feedforward_global, dropout_global,\n","        activation_fn_str_global, max_seq_len_global\n","    ).to(device_global)\n","    try:\n","        inference_model.load_state_dict(torch.load(best_model_weights_path, map_location=device_global))\n","    except RuntimeError as e:\n","        print(f\"Model state_dict yÃ¼klenirken hata: {e}\\nBEST_MODEL_PATH'teki state_dict, mevcut model mimarisiyle uyumsuz olabilir.\")\n","        return\n","\n","    inference_model.eval()\n","    num_to_sample = min(num_samples_to_infer, len(validation_pairs_inf))\n","    if num_to_sample == 0: print(\"Ã‡Ä±karÄ±m iÃ§in yeterli Ã¶rnek yok.\"); return\n","\n","    sample_indices = random.sample(range(len(validation_pairs_inf)), num_to_sample)\n","    for i_idx, original_idx in enumerate(sample_indices):\n","        src_raw, trg_raw_reference = validation_pairs_inf[original_idx]\n","        print(f\"\\nÃ–rnek {i_idx + 1} (Validasyon setinden {original_idx}. indeks):\")\n","        print(f\"  Kaynak (Ä°ngilizce): {src_raw}\\n  Referans (TÃ¼rkÃ§e) : {trg_raw_reference}\")\n","        predicted_trg_sentence = translate_single_sentence_for_validation(\n","            inference_model, src_raw, src_tokenizer_global, trg_tokenizer_global,\n","            device_global, max_seq_len_global)\n","        print(f\"  Model Ã‡evirisi    : {predicted_trg_sentence}\")\n","\n","# Ana EÄŸitim AkÄ±ÅŸÄ±\n","if __name__ == '__main__':\n","    start_epoch = 1 # EÄŸitim her zaman 1. epoch'tan baÅŸlar\n","    best_val_loss_for_saving_best_model = float('inf')\n","    epochs_no_improve = 0\n","\n","    print(f\"EÄŸitime {start_epoch}. epoch'tan baÅŸlanÄ±yor...\")\n","    for epoch in range(start_epoch, NUM_EPOCHS + 1):\n","        epoch_start_time = time.time()\n","        avg_train_loss = train_epoch(model, train_dataloader, optimizer, criterion, CLIP_GRAD)\n","        avg_val_loss = float('inf')\n","        if val_dataloader and len(val_dataloader) > 0:\n","            avg_val_loss = evaluate(model, val_dataloader, criterion)\n","        epoch_duration = time.time() - epoch_start_time\n","\n","        val_loss_display_str = f\"{avg_val_loss:.4f}\" if val_dataloader and len(val_dataloader) > 0 and avg_val_loss != float('inf') else \"N/A\"\n","        log_message = (f\"Epoch {epoch}/{NUM_EPOCHS} | SÃ¼re: {epoch_duration:.2f}s | \"\n","                       f\"EÄŸitim K: {avg_train_loss:.4f} | \")\n","        if val_dataloader and len(val_dataloader) > 0 :\n","            log_message += f\"Val K: {val_loss_display_str} | \"\n","        log_message += f\"LR: {optimizer.param_groups[0]['lr']:.2e}\"\n","        print(log_message)\n","\n","        current_lr_before_step = optimizer.param_groups[0]['lr']\n","        if scheduler:\n","            if val_dataloader and len(val_dataloader) > 0 and avg_val_loss != float('inf'):\n","                scheduler.step(avg_val_loss)\n","            elif not (val_dataloader and len(val_dataloader) > 0) :\n","                scheduler.step(avg_train_loss)\n","        if optimizer.param_groups[0]['lr'] < current_lr_before_step:\n","            print(f\"  Epoch {epoch}: Ã–ÄŸrenme oranÄ± {current_lr_before_step:.2e} -> {optimizer.param_groups[0]['lr']:.2e}\")\n","\n","        if val_dataloader and len(val_dataloader) > 0 and avg_val_loss != float('inf'):\n","            if avg_val_loss < best_val_loss_for_saving_best_model - EARLY_STOPPING_MIN_DELTA:\n","                best_val_loss_for_saving_best_model = avg_val_loss\n","                epochs_no_improve = 0\n","                torch.save(model.state_dict(), BEST_MODEL_PATH)\n","                print(f\"  âœ¨ Yeni en iyi model kaydedildi: {BEST_MODEL_FILE_NAME} (Val K: {best_val_loss_for_saving_best_model:.4f})\")\n","            else:\n","                epochs_no_improve += 1\n","\n","        checkpoint_data = {\n","            'epoch': epoch, 'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n","            'src_word2idx': SRC_TOKENIZER.word2idx, 'trg_word2idx': TRG_TOKENIZER.word2idx,\n","            'd_model': current_d_model, 'nhead': current_n_head,\n","            'num_encoder_layers': current_num_encoder_layers, 'num_decoder_layers': current_num_decoder_layers,\n","            'dim_feedforward': current_dim_feedforward, 'dropout': current_dropout,\n","            'activation_fn_str': current_activation_fn_str, 'max_seq_len': current_max_seq_len,\n","            'pad_idx': PAD_IDX,\n","            'best_val_loss': best_val_loss_for_saving_best_model, # Kaydedilen en iyi val kaybÄ±\n","            'epochs_no_improve': epochs_no_improve # O anki iyileÅŸmeme sayÄ±sÄ±\n","        }\n","        torch.save(checkpoint_data, LAST_CHECKPOINT_PATH)\n","        # Her epoch loglandÄ±ÄŸÄ± iÃ§in, checkpoint kaydÄ±nÄ± daha az sÄ±klÄ±kta loglayabiliriz.\n","        if epoch % 10 == 0 or epoch == 1 or epoch == NUM_EPOCHS :\n","             print(f\"  Checkpoint kaydedildi: {LAST_CHECKPOINT_PATH}\")\n","\n","        if val_dataloader and len(val_dataloader) > 0:\n","            if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n","                print(f\"ðŸ”´ Erken Durdurma: Val kaybÄ± {EARLY_STOPPING_PATIENCE} epoch boyunca iyileÅŸmedi. En iyi Val K: {best_val_loss_for_saving_best_model:.4f}\")\n","                break\n","\n","    print(\"\\nâœ… EÄŸitim TamamlandÄ±.\")\n","    print(f\"Son checkpoint: {LAST_CHECKPOINT_PATH}\")\n","    if os.path.exists(BEST_MODEL_PATH):\n","        print(f\"En iyi model '{BEST_MODEL_FILE_NAME}': {BEST_MODEL_PATH} (Val K: {best_val_loss_for_saving_best_model:.4f})\")\n","    else:\n","        print(f\"UYARI: En iyi model ({BEST_MODEL_PATH}) kaydedilmedi.\")\n","\n","    if os.path.exists(BEST_MODEL_PATH) and val_pairs:\n","        run_inference_on_validation_samples(\n","            num_samples_to_infer=5, validation_pairs_inf=val_pairs, best_model_weights_path=BEST_MODEL_PATH,\n","            src_tokenizer_global=SRC_TOKENIZER, trg_tokenizer_global=TRG_TOKENIZER, pad_idx_global=PAD_IDX,\n","            d_model_global=current_d_model, n_head_global=current_n_head,\n","            num_encoder_layers_global=current_num_encoder_layers, num_decoder_layers_global=current_num_decoder_layers,\n","            dim_feedforward_global=current_dim_feedforward, dropout_global=current_dropout,\n","            activation_fn_str_global=current_activation_fn_str, max_seq_len_global=current_max_seq_len,\n","            device_global=DEVICE)\n","    elif not val_pairs: print(\"DoÄŸrulama seti yok, Ã§Ä±karÄ±m yapÄ±lamÄ±yor.\")\n","    elif not os.path.exists(BEST_MODEL_PATH): print(f\"En iyi model ({BEST_MODEL_PATH}) yok, Ã§Ä±karÄ±m yapÄ±lamÄ±yor.\")"]}]}