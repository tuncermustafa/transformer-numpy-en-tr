{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyMZvTFyT0qN+yNoiwloy5E5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ma-hTHl1mSL1","executionInfo":{"status":"ok","timestamp":1749208433287,"user_tz":-180,"elapsed":3255134,"user":{"displayName":"Mustafa Tunçer","userId":"09475384975611862473"}},"outputId":"fe111a3e-6ee5-44ba-d3b5-2e4358d80dc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","--- BLEU Skoru Değerlendirmesi Başlatılıyor ---\n","Model yapılandırması ve tokenizer'lar yükleniyor...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-11-46c402a060b0>:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(CONFIG_CHECKPOINT_PATH, map_location='cpu')\n"]},{"output_type":"stream","name":"stdout","text":["En iyi model ağırlıkları yükleniyor...\n","Model başarıyla yüklendi.\n","47304 cümle için çeviri üretiliyor... Bu işlem uzun sürebilir.\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/47304 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n","  output = torch._nested_tensor_from_mask(\n","100%|██████████| 47304/47304 [54:00<00:00, 14.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","==================================================\n","               BLEU SKORU SONUCU\n","==================================================\n","BLEU = 46.13 70.4/52.1/39.9/30.9 (BP = 1.000 ratio = 1.045 hyp_len = 236179 ref_len = 226086)\n","==================================================\n"]}],"source":["import torch\n","import torch.nn as nn\n","import os\n","import math\n","from typing import List, Tuple\n","# !pip install sacrebleu tqdm -q\n","import sacrebleu\n","from tqdm import tqdm\n","\n","# Google Drive'a bağlan\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# --- Model ve Tokenizer Sınıf Tanımları ---\n","class WordTokenizer:\n","    def __init__(self):\n","        self.word2idx = {}\n","        self.idx2word = {}\n","        self.vocab_size = 0\n","    def encode(self, sentence: str, add_sos_eos: bool = False) -> List[int]:\n","        tokens = [self.word2idx.get(word, self.word2idx['<unk>']) for word in sentence.lower().split()]\n","        if add_sos_eos: tokens = [self.word2idx['<sos>']] + tokens + [self.word2idx['<eos>']]\n","        return tokens\n","    def decode(self, indices: List[int]) -> str:\n","        return ' '.join([self.idx2word.get(idx, '<unk>') for idx in indices if idx not in [self.word2idx['<pad>'], self.word2idx['<sos>'], self.word2idx['<eos>']]])\n","\n","class Embedding(nn.Module):\n","    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int = None):\n","        super().__init__()\n","        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return self.embedding(x)\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=100):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)[:,:d_model//2] if d_model % 2 != 0 else torch.cos(position * div_term)\n","        self.register_buffer('pe', pe.unsqueeze(0))\n","    def forward(self, x):\n","        return self.dropout(x + self.pe[:, :x.size(1)])\n","\n","class Transformer(nn.Module):\n","    def __init__(self, src_vocab_size: int, trg_vocab_size: int, src_pad_idx: int, trg_pad_idx: int,\n","                 d_model: int, nhead: int, num_encoder_layers: int, num_decoder_layers: int,\n","                 dim_feedforward: int, dropout: float, activation_fn_str: str, max_seq_len: int):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.src_pad_idx = src_pad_idx\n","        self.trg_pad_idx = trg_pad_idx\n","        self.src_embedding = Embedding(src_vocab_size, d_model, padding_idx=src_pad_idx)\n","        self.trg_embedding = Embedding(trg_vocab_size, d_model, padding_idx=trg_pad_idx)\n","        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=max_seq_len)\n","        self.transformer = nn.Module()\n","        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation_fn_str, batch_first=True, norm_first=False)\n","        self.transformer.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers, nn.LayerNorm(d_model))\n","        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation_fn_str, batch_first=True, norm_first=False)\n","        self.transformer.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers, nn.LayerNorm(d_model))\n","        self.fc_out = nn.Linear(d_model, trg_vocab_size)\n","    def generate_square_subsequent_mask(self, sz: int, device: torch.device) -> torch.Tensor:\n","        return torch.triu(torch.full((sz, sz), float('-inf'), device=device, dtype=torch.float32), diagonal=1)\n","    def create_padding_mask(self, seq: torch.Tensor, pad_idx: int) -> torch.Tensor:\n","        return (seq == pad_idx)\n","    def forward(self, src: torch.Tensor, trg_input: torch.Tensor) -> torch.Tensor:\n","        src_emb = self.pos_encoder(self.src_embedding(src) * math.sqrt(self.d_model))\n","        trg_emb = self.pos_encoder(self.trg_embedding(trg_input) * math.sqrt(self.d_model))\n","        src_key_padding_mask_bool = self.create_padding_mask(src, self.src_pad_idx)\n","        trg_key_padding_mask_bool = self.create_padding_mask(trg_input, self.trg_pad_idx)\n","        tgt_attn_mask_float = self.generate_square_subsequent_mask(trg_input.size(1), device=trg_input.device)\n","        src_key_padding_mask_float = torch.zeros_like(src_key_padding_mask_bool, dtype=src_emb.dtype).masked_fill_(src_key_padding_mask_bool, float('-inf'))\n","        trg_key_padding_mask_float = torch.zeros_like(trg_key_padding_mask_bool, dtype=trg_emb.dtype).masked_fill_(trg_key_padding_mask_bool, float('-inf'))\n","        memory = self.transformer.encoder(src_emb, mask=None, src_key_padding_mask=src_key_padding_mask_float)\n","        output = self.transformer.decoder(trg_emb, memory, tgt_mask=tgt_attn_mask_float, memory_mask=None,\n","                                          tgt_key_padding_mask=trg_key_padding_mask_float,\n","                                          memory_key_padding_mask=src_key_padding_mask_float)\n","        return self.fc_out(output)\n","\n","@torch.no_grad()\n","def translate_single_sentence(\n","    inference_model: Transformer, sentence_str: str, src_tokenizer_inf: WordTokenizer,\n","    trg_tokenizer_inf: WordTokenizer, device_inf: torch.device, max_len_inf: int):\n","    inference_model.eval()\n","    src_tokens = src_tokenizer_inf.encode(sentence_str.strip().lower(), add_sos_eos=True)\n","    src_tensor = torch.LongTensor(src_tokens).unsqueeze(0).to(device_inf)\n","    trg_indices = [trg_tokenizer_inf.word2idx['<sos>']]\n","    for _ in range(max_len_inf):\n","        trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(device_inf)\n","        output_logits = inference_model(src_tensor, trg_tensor)\n","        pred_token = output_logits.argmax(2)[:, -1].item()\n","        trg_indices.append(pred_token)\n","        if pred_token == trg_tokenizer_inf.word2idx['<eos>']: break\n","    return trg_tokenizer_inf.decode(trg_indices)\n","\n","def load_sentence_pairs_for_eval(filepath: str) -> tuple[list[str], list[str]]:\n","    sources, targets = [], []\n","    with open(filepath, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            parts = line.strip().split('\\t')\n","            if len(parts) == 2:\n","                sources.append(parts[0])\n","                targets.append(parts[1])\n","    return sources, targets\n","\n","def run_bleu_evaluation():\n","    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    DRIVE_BASE_PATH = '/content/drive/My Drive/transformer/'\n","    CHECKPOINT_DIR = os.path.join(DRIVE_BASE_PATH, 'model_checkpoints')\n","\n","    CONFIG_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'last_checkpoint_512.pth')\n","    BEST_MODEL_WEIGHTS_PATH = os.path.join(CHECKPOINT_DIR, 'best_model_512.pth')\n","    VALIDATION_FILE_PATH = os.path.join(DRIVE_BASE_PATH, 'en2tr_valid.txt')\n","\n","    print(\"--- BLEU Skoru Değerlendirmesi Başlatılıyor ---\")\n","    if not all(os.path.exists(p) for p in [CONFIG_CHECKPOINT_PATH, BEST_MODEL_WEIGHTS_PATH, VALIDATION_FILE_PATH]):\n","        print(\"HATA: Gerekli dosyalardan biri bulunamadı. Yolları kontrol edin.\")\n","        return\n","\n","    # 1. Yapılandırmayı ve tokenizer'ları yükle\n","    print(\"Model yapılandırması ve tokenizer'lar yükleniyor...\")\n","    checkpoint = torch.load(CONFIG_CHECKPOINT_PATH, map_location='cpu')\n","\n","    src_tokenizer = WordTokenizer(); src_tokenizer.word2idx = checkpoint['src_word2idx']; src_tokenizer.idx2word = {v:k for k,v in checkpoint['src_word2idx'].items()}\n","    trg_tokenizer = WordTokenizer(); trg_tokenizer.word2idx = checkpoint['trg_word2idx']; trg_tokenizer.idx2word = {v:k for k,v in checkpoint['trg_word2idx'].items()}\n","\n","    # Transformer'ın ihtiyaç duyduğu anahtarları bir listede tanımlayalım\n","    model_architecture_keys = [\n","        'd_model', 'nhead', 'num_encoder_layers', 'num_decoder_layers',\n","        'dim_feedforward', 'dropout', 'activation_fn_str', 'max_seq_len'\n","    ]\n","    # Sadece bu anahtarları checkpoint'ten alarak config sözlüğünü oluşturalım\n","    config = {key: checkpoint[key] for key in model_architecture_keys if key in checkpoint}\n","\n","    pad_idx = checkpoint.get('pad_idx', 0)\n","    config['src_pad_idx'] = pad_idx\n","    config['trg_pad_idx'] = pad_idx\n","\n","    # 2. Modeli oluştur ve en iyi ağırlıkları yükle\n","    print(\"En iyi model ağırlıkları yükleniyor...\")\n","    model = Transformer(\n","        src_vocab_size=len(src_tokenizer.word2idx),\n","        trg_vocab_size=len(trg_tokenizer.word2idx),\n","        **config\n","    ).to(DEVICE)\n","\n","    model.load_state_dict(torch.load(BEST_MODEL_WEIGHTS_PATH, map_location=DEVICE, weights_only=True))\n","    model.eval()\n","\n","    print(\"Model başarıyla yüklendi.\")\n","    sources, references = load_sentence_pairs_for_eval(VALIDATION_FILE_PATH)\n","    print(f\"{len(sources)} cümle için çeviri üretiliyor... Bu işlem uzun sürebilir.\")\n","    hypotheses = [translate_single_sentence(model, src, src_tokenizer, trg_tokenizer, DEVICE, config.get('max_seq_len', 50)) for src in tqdm(sources)]\n","    bleu = sacrebleu.corpus_bleu(hypotheses, [references])\n","\n","    print(\"\\n\" + \"=\"*50); print(\" \" * 15 + \"BLEU SKORU SONUCU\"); print(\"=\"*50)\n","    print(bleu)\n","    print(\"=\"*50)\n","\n","run_bleu_evaluation()\n"]}]}